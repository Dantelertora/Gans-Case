{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSQ5Fev3rZ2M"
   },
   "source": [
    "# Web scraping\n",
    "\n",
    "Web scraping is a technique used to extract data from websites. It involves sending HTTP requests to websites, parsing the returned HTML code, and extracting the desired data. Web scraping is a powerful tool for data scientists as it allows them to collect large amounts of data from the web. This data can then be used to train machine learning models, analyse trends, and make informed business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktDmVq208N_-"
   },
   "source": [
    "---\n",
    "## 1.&nbsp; Import libraries üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1704718063628,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "wL77EPUT7tc6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xg2dCwV69i6u"
   },
   "source": [
    "---\n",
    "## 2.&nbsp; Beautiful Soup üç≤\n",
    "\n",
    "[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a Python library that simplifies the process of web scraping. It provides a user-friendly interface for parsing HTML documents, enabling users to extract specific information from websites. Through Beautiful Soup, you can navigate the HTML tree structure, locate elements based on their tags, attributes, and content, and extract the desired data into a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWYO44DoBamu"
   },
   "source": [
    "To illustrate how to use Beautiful Soup, we'll use the simplified mock website below. This stripped-down version serves as a practical learning tool, as real websites often possess much larger and more complex HTML structures. By starting with this simplified model, you can gradually build your skills and expertise, ensuring a solid understanding of the core concepts before tackling more intricate web scraping tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1704718126925,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "jl6v_XhL1t0-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\" meta=\"Middle sister\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\" meta=\"Youngest sister\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfSTZsHAClN-"
   },
   "source": [
    "\n",
    "Beautiful Soup's HTML parser takes the raw, unruly HTML code and transforms it into a neatly organised tree structure, making the information easily accessible and manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1704718594373,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "uXuMlTd014SF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoPKtzURCmMu"
   },
   "source": [
    "We can see the tree structure using Beautiful Soup's `.prettify` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1704718595049,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "yxpOiIou-KwF",
    "outputId": "03820296-d245-4dcc-e732-cb467276ce58",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\" meta=\"Middle sister\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\" meta=\"Youngest sister\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXAo3MBl8oy9"
   },
   "source": [
    "---\n",
    "## 3.&nbsp; Navigating html for beginners üß≠\n",
    "There are many methods in Beautiful Soup to explore the html data. By far the most popular and useful of these is .find_all(). So, naturally, this is where we'll start our journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLDnNzZ08scc"
   },
   "source": [
    "### 3.1.&nbsp; `.find_all()`\n",
    "The `.find_all()` method in Beautiful Soup returns a list of all the elements that match the specified criteria, such as tag name, class name, or attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4eVS1rUAp9D"
   },
   "source": [
    "#### 3.1.1.&nbsp; Searching by tag\n",
    "\n",
    "The tags are the letter/word at the beginning of the angle brackets. For example, below, these brackets have an `a` tag.\n",
    "\n",
    "`<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>`\n",
    "\n",
    "The `.find_all()` method takes a string argument and returns a list of all matching HTML tags within the current document. If no matching tags exist, an empty list is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1704718183546,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "TJFWDbHmf6ur",
    "outputId": "a2028c6f-e087-435b-89d6-dbf2dac6621e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1704718192289,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "qqrujkXd8qK5",
    "outputId": "c09cb354-c90f-4f3b-d054-1733b7c4507e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\"><b>The Dormouse's story</b></p>,\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\" meta=\"Middle sister\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\" meta=\"Youngest sister\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>,\n",
       " <p class=\"story\">...</p>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7elwgBfA5GW"
   },
   "source": [
    "#### 3.1.2.&nbsp; Searching by attribute\n",
    "\n",
    "Attributes are the other information in the angle brackets. For example, below, these brackets have a `class`, `href`, `id`, and `meta` attribute.\n",
    "\n",
    "`<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>`\n",
    "\n",
    "Attributes provide additional context and functionality to the elements. They can serve various purposes, including CSS selectors for styling, URLs for linking to external resources, metadata for storing relevant data, and a multitude of other information-bearing components. By leveraging these attributes, we can effectively target specific sections of the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4SZY8Z8hAA0"
   },
   "source": [
    "##### 3.1.2.1.&nbsp; CSS selectors\n",
    "CSS selectors are used to to style certain sections of websites. This makes them very helpful for webscraping as we can then target certain regions of the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yoarw3ghEe0"
   },
   "source": [
    "###### 3.1.2.1.1.&nbsp; Class\n",
    "Class selectors are used to style **multiple** HTML elements that share a common characteristic or function.\n",
    "> **Note:** here class has an underscore at the end of the word, this is because class is a reserved keyword in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1704718249577,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "r_amindGhC13",
    "outputId": "1f28e9bd-7141-4f1f-b721-8f65b10ffdea",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\" meta=\"Middle sister\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\" meta=\"Youngest sister\">Tillie</a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_=\"sister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfFFv08whDET"
   },
   "source": [
    "###### 3.1.2.1.2.&nbsp; ID\n",
    "ID selectors are used to style **single** HTML elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1704718064461,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "tZRqX1fhhC4h",
    "outputId": "3705b5e9-48dd-425c-d391-307a4d8a1dba",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=\"link1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1704718064461,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "mUmznwLq2Wbi",
    "outputId": "4cad0204-8e31-4787-9c44-928e4811f58d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\" meta=\"Middle sister\">Lacie</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=\"link2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrh1aU9f-7z4"
   },
   "source": [
    "##### 3.1.2.2.&nbsp; Other attributes\n",
    "HTML elements can also include other attributes, which can be equally useful for identifying and targeting specific data points. To locate these attributes, search for them using the same method as you do for CSS selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1704718064462,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "A3leok25jS5t",
    "outputId": "75113210-8ca9-4f75-ecde-375508978f7b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\" meta=\"Youngest sister\">Tillie</a>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(meta=\"Youngest sister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBKjBGRHBHds"
   },
   "source": [
    "#### 3.1.3.&nbsp; Searching by string\n",
    "The text (string) is the part between the opening and closing angle brackets, this is what's displayed on the webpage. For example, below, these brackets have `Elsie` as the text.\n",
    "\n",
    "`<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>`\n",
    "\n",
    "Instead of searching for specific tags or attributes, you can also search for this text. To do this, you can use a string or a regular expression to specify the text you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1704718616601,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "aBMjrtNJJcJJ",
    "outputId": "21d5bbc5-eeb0-4379-dc08-7e3bb9d49803",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\" meta=\"Middle sister\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\" meta=\"Youngest sister\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1704718631975,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "J6JWfJ65A_tl",
    "outputId": "d4eb0753-e522-4b04-f4fb-f129e3e234cd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(string=\"Dormouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tilKV8RJX6Sl"
   },
   "source": [
    "The string \"Dormouse\" didn't return any results because BeautifulSoup searches for entire strings that exactly match the string you entered. In other words, the string must be the exact same as what you're searching for for it to be considered a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1704718064462,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "JgD4tC63CMcM",
    "outputId": "95ac2fea-04e1-493d-d3ce-62c741dbec9f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\", \"The Dormouse's story\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(string=\"The Dormouse's story\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji8otCeHYT0-"
   },
   "source": [
    "To search for a substring, the easiest way is to use the regular expressions method `.compile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1704718064462,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "3a-9m5zNmxok",
    "outputId": "ab27fece-850d-42ba-bd6d-a90df26db4e7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\", \"The Dormouse's story\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "soup.find_all(string=re.compile(\"dormouse\", re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEZWgddkZJkx"
   },
   "source": [
    "> **Note:** by default, the .compile() method is case-sensitive, meaning it will only match strings that are exactly equal to the pattern you specify, including case. To perform case-insensitive matching, you must explicitly pass the re.IGNORECASE flag to the .compile() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ROMgD-y9mZU"
   },
   "source": [
    "### 3.2.&nbsp; Extracting text\n",
    "There are a few ways to extract text in Beautiful Soup, here we'll focus on 2 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs9mUvTI9oZN"
   },
   "source": [
    "#### 3.2.1.&nbsp; `.get_text()`\n",
    "The `.get_text()` method extracts all the human-readable text from a Beautiful Soup object, returning it as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1704718064463,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "_x-R0WQm9n-L",
    "outputId": "fef20502-a9ca-41be-86cc-f4dbd87ebc5d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "error",
     "timestamp": 1704718064463,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "T4aPfFuT4CUi",
    "outputId": "d9d0ed3c-45ab-4b0b-e6b2-874ada28ba36",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"title\")[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOgNw4TAAbXj"
   },
   "source": [
    "> Read the error message and look at the output from the cell above. Can you work out why we got an error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1704718064464,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "mgI1c4Ts4O67"
   },
   "outputs": [],
   "source": [
    "# @title Click `show code` to see the solution to the error\n",
    "\n",
    "# It was a list, read the error messages and notice the square brackets in the original output\n",
    "# Therefore, we need to select the first and only element of this list\n",
    "soup.find_all(\"title\")[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ9rd5VgCDhz"
   },
   "source": [
    "We can also print out multiple items using our looping skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1704718064464,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "oGru8IYF4SQn",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\"><b>The Dormouse's story</b></p>,\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\" meta=\"Middle sister\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\" meta=\"Youngest sister\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>,\n",
       " <p class=\"story\">...</p>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = soup.find_all(\"p\")\n",
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1704718064464,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "HG6TMWf54X3N",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for p in story:\n",
    "  print(p.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUh2ZYo-9x-S"
   },
   "source": [
    "#### 3.2.2.&nbsp; Extracting attributes:\n",
    "HTML elements often store additional information within their attributes. To extract this data using Beautiful Soup, you can append square brackets after the element selector and specify the attribute name within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1704718064464,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "xdqcP4nHECTF",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\" meta=\"Eldest sister\">Elsie</a>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=\"link1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1704718064465,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "4x4t_mqjDgqb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://example.com/elsie'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=\"link1\")[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 2151,
     "status": "aborted",
     "timestamp": 1704718064466,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "7uIkOhD1PmlX",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eldest sister'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=\"link1\")[0]['meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVZJOh1_l0Rt"
   },
   "source": [
    "## Challenge 1 üòÄ\n",
    "Below is new HTML code. Use your scrapping skills to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2150,
     "status": "aborted",
     "timestamp": 1704718064466,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "7lAHzu1Dl2X0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "geography = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head> Geography</head>\n",
    "<body>\n",
    "\n",
    "<div class=\"city\">\n",
    "  <h2>London</h2>\n",
    "  <p>London is the most popular tourist destination in the world.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"city\">\n",
    "  <h2>Paris</h2>\n",
    "  <p>Paris was originally a Roman City called Lutetia.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"country\">\n",
    "  <h2>Spain</h2>\n",
    "  <p>Spain produces 43,8% of all the world's Olive Oil.</p>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2149,
     "status": "aborted",
     "timestamp": 1704718064467,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "1KOBMEQNl9EZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  Geography\n",
      " </head>\n",
      " <body>\n",
      "  <div class=\"city\">\n",
      "   <h2>\n",
      "    London\n",
      "   </h2>\n",
      "   <p>\n",
      "    London is the most popular tourist destination in the world.\n",
      "   </p>\n",
      "  </div>\n",
      "  <div class=\"city\">\n",
      "   <h2>\n",
      "    Paris\n",
      "   </h2>\n",
      "   <p>\n",
      "    Paris was originally a Roman City called Lutetia.\n",
      "   </p>\n",
      "  </div>\n",
      "  <div class=\"country\">\n",
      "   <h2>\n",
      "    Spain\n",
      "   </h2>\n",
      "   <p>\n",
      "    Spain produces 43,8% of all the world's Olive Oil.\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the \"soup\"\n",
    "soup = BeautifulSoup(geography, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2147,
     "status": "aborted",
     "timestamp": 1704718064467,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "e7XQtTzcl9B0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London is the most popular tourist destination in the world.\n",
      "Paris was originally a Roman City called Lutetia.\n",
      "Spain produces 43,8% of all the world's Olive Oil.\n"
     ]
    }
   ],
   "source": [
    "# 1. All the \"fun facts\"\n",
    "all_p = soup.find_all('p')\n",
    "for p in all_p:\n",
    "  print(p.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065148,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "ucD8rudDl8_L",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "Paris\n",
      "Spain\n"
     ]
    }
   ],
   "source": [
    "# 2. The names of all the places.\n",
    "all_h = soup.find_all(\"h2\")\n",
    "for h in all_h:\n",
    "  print(h.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1704718065148,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "0K0UQ2V4l88n",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "London\n",
      "London is the most popular tourist destination in the world.\n",
      "\n",
      "\n",
      "Paris\n",
      "Paris was originally a Roman City called Lutetia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. All the content (name and fact) of all the cities (only cities, not countries!)\n",
    "cities = soup.find_all(class_=\"city\")\n",
    "for city in cities:\n",
    "  print(city.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1704718065148,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "cNyiqAEll85q",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "# 4. The names (not facts!) of all the cities (not countries!)\n",
    "cities = soup.find_all(class_=\"city\")\n",
    "for i in cities:\n",
    "    print(i.find_all(\"h2\")[0].get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgq5kkTFk8Dg"
   },
   "source": [
    "---\n",
    "## 4.&nbsp; Navigating html with a few more advanced techniques üó∫Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhTryrEV8tu2"
   },
   "source": [
    "### 4.1.&nbsp; `.find()`\n",
    "`.find()` is similar to `.find_all()`, but it returns only the first element that matches the specified criteria. This makes it useful when you know exactly where the element you're looking for is located and you only need to retrieve one instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "wZjtVNi38sPY",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'London is the most popular tourist destination in the world.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOseQNrW8ueb"
   },
   "source": [
    "### 4.2.&nbsp; `.select()`\n",
    "`.select()` is similar to `.find_all()`, but there are 2 main differences:\n",
    "- the way we write our query in the brackets is slightly different\n",
    "- `.select()` allows you to chain CSS selectors together to navigate through the HTML structure, enabling you to select elements based on their positions within nested elements or patterns. This makes it particularly useful for extracting data from complex HTML structures.\n",
    "\n",
    "In contrast, `.find_all()` uses a simpler syntax based on tag names and attributes, making it more straightforward for basic element selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlLEdmYJK6hB"
   },
   "source": [
    "Here's how we query with `.find_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "PPplvgpe8sL_",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a', class_='sister')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZVv18muK-rK"
   },
   "source": [
    "Here's the same query with `.select()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "Rhc1_2IJJGrz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a.sister')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg8OUGS9LC42"
   },
   "source": [
    "To demonstrate the power of `.select()` in navigating through nested elements, let's extract all the `<a>` tags with the id `'link2'` that are within `<p>` tags with the class `'story'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "BfBvxRUzJMgM"
   },
   "outputs": [],
   "source": [
    "soup.select('p.story a#link2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0tGSzydlcjt"
   },
   "source": [
    "### 4.3.&nbsp; Navigating to the Next or Previous Element\n",
    "In some cases, you may need to access specific elements that are closely related to others, but their HTML structure doesn't provide unique identifiers. To overcome this challenge, you can utilise the `.find_next()` and `.find_previous()` methods to navigate through the HTML structure and reach the desired element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "bWBJnwKh6xAM"
   },
   "outputs": [],
   "source": [
    "last_link = soup.find(id='link3')\n",
    "last_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ox3xJJOP0fhD"
   },
   "source": [
    "#### 4.1.1.&nbsp; `.find_next()`\n",
    "`.find_next()` moves forward one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "zrBfI5c40fTE"
   },
   "outputs": [],
   "source": [
    "last_link.find_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMGziu0x0klO"
   },
   "source": [
    "#### 4.2.&nbsp; `.find_previous()`\n",
    "`.find_previous()` moves back one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "hKMSnJKgwjO3"
   },
   "outputs": [],
   "source": [
    "last_link.find_previous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGSarkvdVkpj"
   },
   "source": [
    "---\n",
    "## 5.&nbsp; Showcasing these skills on a real website üíª\n",
    "Let's see what information we can get from the wikipedia site for web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ztc-9lpTm2e9"
   },
   "source": [
    "### Loading the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "h-qyiClnVptT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup_3 = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo-b2kLwNOWW"
   },
   "source": [
    "> While we haven't yet looked into the requests library, we'll postpone delving into it today to avoid overwhelming you with too much new information. Instead, we'll explore the requests library when we start gathering weather data later in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wBsdqjum_o_"
   },
   "source": [
    "### Getting the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "2V9-cuYsWVLP",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web scraping - Wikipedia'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_3.find(\"title\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGZWvJFmnLZp"
   },
   "source": [
    "### Getting the first h1 tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065149,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "QejF-CVDWdVE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web scraping'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_3.find(\"h1\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0bLXLH-nNyY"
   },
   "source": [
    "### Getting all the h2 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1704718065150,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "Jcx15L3XWk98",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"vector-pinnable-header-label\">Contents</h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"History\">History</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=1\" title=\"Edit section: History\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Techniques\">Techniques</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=2\" title=\"Edit section: Techniques\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Software\">Software</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=11\" title=\"Edit section: Software\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Legal_issues\">Legal issues</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=12\" title=\"Edit section: Legal issues\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Methods_to_prevent_web_scraping\">Methods to prevent web scraping</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=17\" title=\"Edit section: Methods to prevent web scraping\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=18\" title=\"Edit section: See also\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=19\" title=\"Edit section: References\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2_tags = soup_3.find_all(\"h2\")\n",
    "h2_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPWXrxtAnSR-"
   },
   "source": [
    "As we have multiple tags in the list here, we need to use a loop to print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1704718065150,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "j-Lw4pCEW7Uy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "History[edit]\n",
      "Techniques[edit]\n",
      "Software[edit]\n",
      "Legal issues[edit]\n",
      "Methods to prevent web scraping[edit]\n",
      "See also[edit]\n",
      "References[edit]\n"
     ]
    }
   ],
   "source": [
    "for h2 in h2_tags:\n",
    "  print(h2.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIyT3sijoGuK"
   },
   "source": [
    "### Selecting the `Legal Issues` text for only `India`\n",
    "> **Pro tip:** If you're using Google Chrome, you can navigate to `View > Developer > Inspect elements` to access the built-in web development tools. Here, you can explore the HTML structure of the webpage directly within the browser using your mouse. This interactive approach is often more intuitive than examining the raw HTML code.\n",
    "\n",
    "By investigating the html we can see that the closest, easy to access, tag is the heading with the CSS `id` of `\"India\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065150,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "VQkUGkeoh0G_",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"mw-headline\" id=\"India\">India</span>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_3.find(id=\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYx5YrJKoh4a"
   },
   "source": [
    "We can then use `.find_next()` to select the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065150,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "lLxrYkO3hbZn",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Web_scraping&amp;action=edit&amp;section=16\" title=\"Edit section: India\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_3.find(id=\"India\").find_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q67eh_Oxo7B2"
   },
   "source": [
    "Looks like the next tag was a `span` tag, so let's specify that we want the next `p` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065150,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "oU-Ai8-Vgdsw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>Leaving a few cases dealing with IPR infringement, Indian courts have not expressly ruled on the legality of web scraping. However, since all common forms of electronic contracts are enforceable in India, violating the terms of use prohibiting data scraping will be a violation of the contract law. It will also violate the <a href=\"/wiki/Information_Technology_Act,_2000#:~:text=From_Wikipedia,_the_free_encyclopedia_The_Information_Technology,in_India_dealing_with_cybercrime_and_electronic_commerce.\" title=\"Information Technology Act, 2000\">Information Technology Act, 2000</a>, which penalizes unauthorized access to a computer resource or extracting data from a computer resource.\n",
       "</p>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_3.find(id=\"India\").find_next(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b53SGH5pFwb"
   },
   "source": [
    "Now we can simply extract the text, and we have what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065150,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "UP5YwO3ii3NR",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leaving a few cases dealing with IPR infringement, Indian courts have not expressly ruled on the legality of web scraping. However, since all common forms of electronic contracts are enforceable in India, violating the terms of use prohibiting data scraping will be a violation of the contract law. It will also violate the Information Technology Act, 2000, which penalizes unauthorized access to a computer resource or extracting data from a computer resource.\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_3.find(id=\"India\").find_next(\"p\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4QQbALMuZ99"
   },
   "source": [
    "## Challenge 2 üòÄ\n",
    "\n",
    "Utilise your web scraping skills to gather information about three German cities ‚Äì Berlin, Hamburg, and Munich ‚Äì from Wikipedia. You will start by extracting the population of each city and then expand the scope of your data gathering to include latitude and longitude, country, and possibly other relevant details.\n",
    "\n",
    "1. Population Scraping\n",
    "\n",
    "  1.1. Begin by scraping the population of each city from their respective Wikipedia pages:\n",
    "\n",
    " - Berlin: https://en.wikipedia.org/wiki/Berlin\n",
    " - Hamburg: https://en.wikipedia.org/wiki/Hamburg\n",
    " - Munich: https://en.wikipedia.org/wiki/Munich\n",
    "\n",
    "  1.2. Once you have scrapped the population of each city, reflect on the similarities and patterns in accessing the population data across the three pages. Also, analyse the URLs to identify any commonalities. Make a loop that executes once but simultaneously retrieves the population for all three cities.\n",
    "\n",
    "2. Data Organisation\n",
    "\n",
    "  Utilise pandas DataFrame to effectively store the extracted population data. Ensure the data is clean and properly formatted. Remove any unnecessary characters or symbols and ensure the column data types are accurate.\n",
    "\n",
    "3. Further Enhancement\n",
    "\n",
    "  3.1. Expand the scope of your data gathering by extracting other relevant information for each city:\n",
    "\n",
    " - Latitude and longitude\n",
    " - Country of location\n",
    "\n",
    "  3.2. Create a function from the loop and DataFrame to encapsulate the scraping process. This function can be used repeatedly to fetch updated data whenever necessary. It should return a clean, properly formatted DataFrame.\n",
    "\n",
    "4. Global Data Scraping\n",
    "\n",
    "  With your robust scraping skills now honed, venture beyond the confines of Germany and explore other cities around the world. While the extraction methodology for German cities may follow a consistent pattern, this may not be the case for cities from different countries. Can you make a function that returns a clean DataFrame of information for cities worldwide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1704718065150,
     "user": {
      "displayName": "dante lertora",
      "userId": "12072860331463149400"
     },
     "user_tz": -60
    },
    "id": "jXI3PQxjwNC1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3850809"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.1 Population Scraping\n",
    "#Calling the URL\n",
    "url = \"https://en.wikipedia.org/wiki/Berlin\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup_3 = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#First step is to approach to the line mentioning \"population\", then finding the next \"td\" tag on the script and converting it into an int\n",
    "\n",
    "population_berlin = soup_3.find(string= re.compile(\"population\", re.IGNORECASE)).find_next(\"td\").get_text()\n",
    "\n",
    "#It is necessary to convert this string into an int removing firstly the \",\" and as a second step turning it into an int\n",
    "population_berlin_int = int(population_berlin.replace(\",\",\"\"))\n",
    "population_berlin_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1945532"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same steps for Hamburg\n",
    "#Calling the URL\n",
    "url = \"https://en.wikipedia.org/wiki/Hamburg\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup_4 = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#First step is to approach to the line mentioning \"population\", then finding the next \"td\" tag on the script and converting it into an int\n",
    "\n",
    "population_hambourg = soup_4.find(string= re.compile(\"population\", re.IGNORECASE)).find_next(\"td\").get_text()\n",
    "population_hambourg\n",
    "\n",
    "#It is necessary to convert this string into an int removing firstly the \",\" and as a second step turning it into an int\n",
    "population_hambourg_int = int(population_hambourg.replace(\",\",\"\"))\n",
    "population_hambourg_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1512491"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same steps for Munich\n",
    "#Calling the URL\n",
    "url = \"https://en.wikipedia.org/wiki/Munich\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup_5 = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#First step is to approach to the line mentioning \"population\", then finding the next \"td\" tag on the script and converting it into an int\n",
    "\n",
    "population_munich = int(soup_5.find(string= re.compile(\"population\", re.IGNORECASE)).find_next(\"td\").get_text().replace(\",\",\"\"))\n",
    "population_munich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3850809, 1945532, 1512491]\n"
     ]
    }
   ],
   "source": [
    "#1.2. Once you have scrapped the population of each city, reflect on the similarities and patterns in accessing the population data across the three pages.\n",
    "#Also, analyse the URLs to identify any commonalities. Make a loop that executes once but simultaneously retrieves the population for all three cities.\n",
    "\n",
    "cities = [\"Berlin\",\"Hambourg\",\"Munich\"]\n",
    "population = []\n",
    "for i in cities:\n",
    "    url_loop = f\"https://en.wikipedia.org/wiki/{i}\"\n",
    "\n",
    "    response = requests.get(url_loop)\n",
    "\n",
    "    soup_6 = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    population.append(int(soup_6.find(string= re.compile(\"population\", re.IGNORECASE)).find_next(\"td\").get_text().replace(\",\",\"\")))\n",
    "\n",
    "print(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berlin</td>\n",
       "      <td>3850809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hambourg</td>\n",
       "      <td>1945532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Munich</td>\n",
       "      <td>1512491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city  population\n",
       "0    Berlin     3850809\n",
       "1  Hambourg     1945532\n",
       "2    Munich     1512491"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Organisation\n",
    "#Utilise pandas DataFrame to effectively store the extracted population data. Ensure the data is clean and properly formatted. \n",
    "#Remove any unnecessary characters or symbols and ensure the column data types are accurate.\n",
    "\n",
    "information = pd.DataFrame({\"city\":cities,\"population\":population})\n",
    "information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>population</th>\n",
       "      <th>latitude</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berlin</td>\n",
       "      <td>3850809</td>\n",
       "      <td>52¬∞31‚Ä≤12‚Ä≥N</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hambourg</td>\n",
       "      <td>1945532</td>\n",
       "      <td>53¬∞33‚Ä≤N</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Munich</td>\n",
       "      <td>1512491</td>\n",
       "      <td>48¬∞08‚Ä≤15‚Ä≥N</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city  population    latitude  country\n",
       "0    Berlin     3850809  52¬∞31‚Ä≤12‚Ä≥N  Germany\n",
       "1  Hambourg     1945532     53¬∞33‚Ä≤N  Germany\n",
       "2    Munich     1512491  48¬∞08‚Ä≤15‚Ä≥N  Germany"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Further Enhancement\n",
    "#3.1. Expand the scope of your data gathering by extracting other relevant information for each city:\n",
    "\n",
    "#Latitude and longitude\n",
    "#Country of location\n",
    "\n",
    "cities = [\"Berlin\",\"Hambourg\",\"Munich\"]\n",
    "population = []\n",
    "latitude = []\n",
    "country = []\n",
    "for i in cities:\n",
    "    url_loop = f\"https://en.wikipedia.org/wiki/{i}\"\n",
    "\n",
    "    response = requests.get(url_loop)\n",
    "\n",
    "    soup_6 = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    population.append(int(soup_6.find(string= re.compile(\"population\", re.IGNORECASE)).find_next(\"td\").get_text().replace(\",\",\"\")))\n",
    "    latitude.append(soup_6.find(class_=\"latitude\").get_text())\n",
    "    country.append(soup_6.find(string= re.compile(\"country\", re.IGNORECASE)).find_next(\"td\").get_text())\n",
    "\n",
    "information = pd.DataFrame({\"city\":cities,\"population\":population,\"latitude\":latitude,\"country\":country})\n",
    "information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48¬∞08‚Ä≤15‚Ä≥N'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_6.find(class_=\"latitude\").get_text()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Germany'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_6.find(string= re.compile(\"country\", re.IGNORECASE)).find_next(\"td\").get_text()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>population</th>\n",
       "      <th>latitude</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berlin</td>\n",
       "      <td>3850809</td>\n",
       "      <td>52¬∞31‚Ä≤12‚Ä≥N</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hambourg</td>\n",
       "      <td>1945532</td>\n",
       "      <td>53¬∞33‚Ä≤N</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Munich</td>\n",
       "      <td>1512491</td>\n",
       "      <td>48¬∞08‚Ä≤15‚Ä≥N</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paris</td>\n",
       "      <td>2102650</td>\n",
       "      <td>48¬∞51‚Ä≤24‚Ä≥N</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>14094034</td>\n",
       "      <td>35¬∞41‚Ä≤23‚Ä≥N</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1620343</td>\n",
       "      <td>41¬∞22‚Ä≤58‚Ä≥N</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Madrid</td>\n",
       "      <td>3223334</td>\n",
       "      <td>40¬∞25‚Ä≤01‚Ä≥N</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Villa_Ballester</td>\n",
       "      <td>35301</td>\n",
       "      <td>34¬∞31‚Ä≤S</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cologne</td>\n",
       "      <td>1073096</td>\n",
       "      <td>50¬∞56‚Ä≤11‚Ä≥N</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              city  population    latitude     country\n",
       "0           Berlin     3850809  52¬∞31‚Ä≤12‚Ä≥N     Germany\n",
       "1         Hambourg     1945532     53¬∞33‚Ä≤N     Germany\n",
       "2           Munich     1512491  48¬∞08‚Ä≤15‚Ä≥N     Germany\n",
       "3            Paris     2102650  48¬∞51‚Ä≤24‚Ä≥N      France\n",
       "4            Tokyo    14094034  35¬∞41‚Ä≤23‚Ä≥N       Japan\n",
       "5        Barcelona     1620343  41¬∞22‚Ä≤58‚Ä≥N       Spain\n",
       "6           Madrid     3223334  40¬∞25‚Ä≤01‚Ä≥N       Spain\n",
       "7  Villa_Ballester       35301     34¬∞31‚Ä≤S  ¬†Argentina\n",
       "8          Cologne     1073096  50¬∞56‚Ä≤11‚Ä≥N     Germany"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next step would be to assure that for many different wikipedia articles, these web scraping will work. A good way to do this is by indicating where to look this information in the first place\n",
    "#Locating the search only on the side box, will assure better results\n",
    "\n",
    "\n",
    "url_loop = \"https://en.wikipedia.org/wiki/Hambourg\"\n",
    "\n",
    "response = requests.get(url_loop)\n",
    "\n",
    "original_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#Only using information of the lateral -> Wont return a list, as find returns a single value\n",
    "side_table = original_soup.find(\"table\", class_=\"infobox ib-settlement vcard\")\n",
    "\n",
    "#Performing the searchs on the already shortened data\n",
    "side_table.find(string = re.compile(\"Population\")).find_next(\"td\").get_text().replace\n",
    "\n",
    "\n",
    "cities = [\"Berlin\",\"Hambourg\",\"Munich\",\"Paris\",\"Tokyo\",\"Barcelona\",\"Madrid\",\"Villa_Ballester\",\"Cologne\"]\n",
    "population = []\n",
    "latitude = []\n",
    "country = []\n",
    "for i in cities:\n",
    "    url_loop = f\"https://en.wikipedia.org/wiki/{i}\"\n",
    "\n",
    "    response = requests.get(url_loop)\n",
    "\n",
    "    original_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    #Only using information of the lateral -> Wont return a list, as find returns a single value\n",
    "    side_table = original_soup.find(\"table\", class_=\"infobox ib-settlement vcard\")\n",
    "\n",
    "    #Performing the searchs on the already shortened data\n",
    "    side_table.find(string = re.compile(\"Population\")).find_next(\"td\").get_text().replace\n",
    "\n",
    "    population.append(int(side_table.find(string= re.compile(\"population\", re.IGNORECASE)).find_next(\"td\").get_text().replace(\",\",\"\")))\n",
    "    latitude.append(side_table.find(class_=\"latitude\").get_text())\n",
    "    country.append(side_table.find(string= re.compile(\"country\", re.IGNORECASE)).find_next(\"td\").get_text())\n",
    "\n",
    "information = pd.DataFrame({\"city\":cities,\"population\":population,\"latitude\":latitude,\"country\":country})\n",
    "information"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1GGtWWr-ckFS_poi1ZbZFd7KDQFhgtMlA",
     "timestamp": 1704705085181
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
